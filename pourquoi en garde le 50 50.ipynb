{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "#from scipy.misc import imresize, imread\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
    "%matplotlib inline\n",
    "# charger des images formes (5547, 50, 50, 3)\n",
    "X = np.load('D:/test cancer rbreast/p/zoom_40m.npy')  \n",
    "\n",
    "# charger des images  (5547,1); (0 = no cancer, 1 = cancer)\n",
    "Y = np.load('D:/test cancer rbreast/p/zoom_40m_label.npy')   \n",
    "perm_array = np.arange(len(X))\n",
    "#arrange genere tous toute les val jusqua la longuer de x_images\n",
    "np.random.shuffle(perm_array)\n",
    "#il melange les images\n",
    "X = X[perm_array]\n",
    "Y = Y[perm_array]\n",
    "#en split notre dataset 80 % 20 %\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# en redduit la taille de l'echantillon au cas ou en avais trop d'images \n",
    "#pour pas que ca prend trop de temps pour le pc \n",
    "#mais ici c'est pas le cas donc en commente ce code\n",
    "#X_train = X_train[0:30000] \n",
    "#Y_train = Y_train[0:30000]\n",
    "#X_test = X_test[0:30000] \n",
    "#Y_test = Y_test[0:30000]\n",
    "X_train = X_train / 256.0\n",
    "X_test = X_test / 256.0\n",
    "#methode qu'on a trouver sur internet et qu'on a utiliser \n",
    "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
    "#en applatit les donnees en gros en les transfrome en une matrice 1 seul dimmenssion nbr elem*(width*height*channel)\n",
    "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
    "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    en va plot un learning curve grace au code de ce lien http://scikit-learn.org/stable/modules/learning_curve.html\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"exemples de test\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"score d'entrainement\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"score du cross validation\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def plotLotsOfLearningCurves(a,b):\n",
    "    \"\"\" en va plot bcp de learning curve http://scikit-learn.org/stable/modules/learning_curve.html\"\"\"\n",
    "    models = []\n",
    "    models.append(('Support Vector Machine', SVC()))\n",
    "    for name, model in models:\n",
    "        plot_learning_curve(model, 'Learning Curve For %s Classifier'% (name), a,b, (0.5,1), 10)\n",
    "#plotLotsOfLearningCurves(X_trainFlat, Y_train)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='matrice d econfusion',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    en affiche la matrice de confusion\n",
    "    en peut normalizer les donnees en metant normalize a true\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#Run SVC w/ Confusion Matrix\n",
    "def runSVCconfusion(a,b,c,d):\n",
    "    \"\"\"methodes qui affiches les matrices de confusiondu model svc (comme exmeple) en normalizer et non normalizer\"\"\"\n",
    "    model = SVC()\n",
    "    model.fit(a, b)\n",
    "    prediction = model.predict(c)\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    accuracy = model_selection.cross_val_score(model, c,d, cv=kfold, scoring='accuracy')\n",
    "    mean = accuracy.mean() \n",
    "    stdev = accuracy.std()\n",
    "    print('\\nSupport Vector Machine - Training set accuracy: %s (%s)' % (mean, stdev),\"\\n\")\n",
    "    cnf_matrix = confusion_matrix(d, prediction)\n",
    "    np.set_printoptions(precision=2)\n",
    "    class_names = [\"diagnostic\" \"IDC(-)\", \"diagnostic\" \"IDC(+)\"]\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, sans la normalization')\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title=' confusion matrix avec normalization')\n",
    "    plt.show()\n",
    "#runSVCconfusion(X_trainFlat, Y_train, X_testFlat, Y_test)\n",
    "Y_train = to_categorical(Y_train, num_classes = 2)\n",
    "Y_test = to_categorical(Y_test, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 3.8549 - acc: 0.6410 - val_loss: 6.4345 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56000, saving model to weights.traint_40zoom_modelcomplex1.hdf5\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.8597 - acc: 0.7490 - val_loss: 3.7631 - val_acc: 0.6960\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.56000 to 0.69600, saving model to weights.traint_40zoom_modelcomplex1.hdf5\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.6486 - acc: 0.7230 - val_loss: 4.3999 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69600\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.5616 - acc: 0.8040 - val_loss: 2.5293 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69600\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.0999 - acc: 0.7740 - val_loss: 5.8562 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69600\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5231 - acc: 0.8460 - val_loss: 1.4668 - val_acc: 0.6320\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69600\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4241 - acc: 0.8570 - val_loss: 1.2673 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69600\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3356 - acc: 0.8710 - val_loss: 0.7204 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.69600 to 0.70800, saving model to weights.traint_40zoom_modelcomplex1.hdf5\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2973 - acc: 0.8730 - val_loss: 1.0801 - val_acc: 0.4720\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.70800\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3009 - acc: 0.8890 - val_loss: 0.3013 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.70800 to 0.85200, saving model to weights.traint_40zoom_modelcomplex1.hdf5\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2271 - acc: 0.9140 - val_loss: 0.5246 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.85200\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2257 - acc: 0.9170 - val_loss: 0.2908 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.85200 to 0.88400, saving model to weights.traint_40zoom_modelcomplex1.hdf5\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1663 - acc: 0.9380 - val_loss: 1.6492 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88400\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1298 - acc: 0.9520 - val_loss: 1.4337 - val_acc: 0.6520\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88400\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2298 - acc: 0.9270 - val_loss: 1.3810 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88400\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1713 - acc: 0.9400 - val_loss: 0.6240 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88400\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1234 - acc: 0.9500 - val_loss: 2.9453 - val_acc: 0.6680\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88400\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1871 - acc: 0.9420 - val_loss: 5.3937 - val_acc: 0.5440\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88400\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2947 - acc: 0.9150 - val_loss: 1.4662 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88400\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1751 - acc: 0.9350 - val_loss: 2.0361 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88400\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1023 - acc: 0.9570 - val_loss: 2.5586 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88400\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0904 - acc: 0.9640 - val_loss: 1.1406 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88400\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1030 - acc: 0.9600 - val_loss: 1.6095 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88400\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0732 - acc: 0.9770 - val_loss: 1.9131 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88400\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0489 - acc: 0.9830 - val_loss: 1.3604 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88400\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1801 - acc: 0.9480 - val_loss: 4.4383 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88400\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1113 - acc: 0.9650 - val_loss: 5.1609 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88400\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0364 - acc: 0.9890 - val_loss: 3.4907 - val_acc: 0.5520\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88400\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0312 - acc: 0.9900 - val_loss: 2.9158 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88400\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0601 - acc: 0.9850 - val_loss: 2.3036 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88400\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0756 - acc: 0.9810 - val_loss: 3.6230 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88400\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0789 - acc: 0.9720 - val_loss: 3.6764 - val_acc: 0.5760\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88400\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0835 - acc: 0.9710 - val_loss: 2.5785 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88400\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0404 - acc: 0.9880 - val_loss: 4.0430 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88400\n",
      "Epoch 35/100\n",
      " 256/1000 [======>.......................] - ETA: 3s - loss: 0.0855 - acc: 0.9766"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8e630282fc09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_characters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m \u001b[0mrunKerasCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;31m#plotKerasLearningCurve()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8e630282fc09>\u001b[0m in \u001b[0;36mrunKerasCNN\u001b[1;34m(a, b, c, d)\u001b[0m\n\u001b[0;32m    104\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m               validation_data=(x_test, y_test),callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "# charger des images formes (5547, 50, 50, 3)\n",
    "X = np.load('D:/test cancer rbreast/p/zoom_40m.npy')  \n",
    "\n",
    "# charger des images  (5547,1); (0 = no cancer, 1 = cancer)\n",
    "Y = np.load('D:/test cancer rbreast/p/zoom_40m_label.npy')   \n",
    "perm_array = np.arange(len(X))\n",
    "#arrange genere tous toute les val jusqua la longuer de x_images\n",
    "np.random.shuffle(perm_array)\n",
    "#il melange les images\n",
    "X = X[perm_array]\n",
    "Y = Y[perm_array]\n",
    "#en split notre dataset 80 % 20 %\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# en redduit la taille de l'echantillon au cas ou en avais trop d'images \n",
    "#pour pas que ca prend trop de temps pour le pc \n",
    "#mais ici c'est pas le cas donc en commente ce code\n",
    "#X_train = X_train[0:30000] \n",
    "#Y_train = Y_train[0:30000]\n",
    "#X_test = X_test[0:30000] \n",
    "#Y_test = Y_test[0:30000]\n",
    "\n",
    "# normalizer nos donnees \n",
    "#en pourrais utiliser d'autre methodes aussi mais celle la c'est bien \n",
    "X_train = X_train / 256.0\n",
    "X_test = X_test / 256.0\n",
    "#methode qu'on a trouver sur internet et qu'on a utiliser \n",
    "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
    "#en applatit les donnees en gros en les transfrome en une matrice 1 seul dimmenssion nbr elem*(width*height*channel)\n",
    "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
    "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\n",
    "Y_train = to_categorical(Y_train, num_classes = 2)\n",
    "Y_test = to_categorical(Y_test, num_classes = 2)\n",
    "def plotKerasLearningCurve():\n",
    "    #en affiche juste la courbe de notre model\n",
    "    plt.figure(figsize=(10,5))\n",
    "    metrics = np.load('logs.npy')[()]\n",
    "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
    "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "        l = np.array(metrics[k])\n",
    "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
    "        y = l[x]\n",
    "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
    "    plt.legend(loc=4)\n",
    "    plt.axis([0, None, None, None]);\n",
    "    plt.grid()\n",
    "    plt.xlabel('nombre d epoques')\n",
    "\n",
    "def runKerasCNN(a,b,c,d):\n",
    "    \"\"\"\n",
    "    en a utiliser ce lien\n",
    "    https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "    en a utilise run model qui marche bien sur MNIST sur notre dataset\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 100 \n",
    "    img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    x_train = a\n",
    "    y_train = b\n",
    "    x_test = c\n",
    "    y_test = d   \n",
    "    model = Sequential()\n",
    "    #test d'un premier model\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu', input_shape = input_shape))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = \"softmax\"))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "     # checkpoint\n",
    "    filepath=\"weights.traint_40zoom_modelcomplex1.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history =model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),callbacks=callbacks_list)\n",
    "    model.summary()\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('\\nKeras CNN 1 - accuracy:', score[1],'\\n')\n",
    "    y_pred = model.predict(c) \n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')\n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "runKerasCNN(X_train, Y_train,  X_test, Y_test)\n",
    "#plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,86,25,25] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_11/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_1/Adadelta/gradients/dropout_7/cond/Merge_grad/cond_grad\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_7/cond/Merge, conv2d_11/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics_1/acc/Mean/_685 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2427_metrics_1/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-86e842b8cd99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_characters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m \u001b[0mrunKerasCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;31m#plotKerasLearningCurve()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-86e842b8cd99>\u001b[0m in \u001b[0;36mrunKerasCNN\u001b[1;34m(a, b, c, d)\u001b[0m\n\u001b[0;32m    104\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m               validation_data=(x_test, y_test),callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,86,25,25] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_11/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_1/Adadelta/gradients/dropout_7/cond/Merge_grad/cond_grad\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_7/cond/Merge, conv2d_11/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics_1/acc/Mean/_685 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2427_metrics_1/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# charger des images formes (5547, 50, 50, 3)\n",
    "X = np.load('D:/test cancer rbreast/p/zoom_40m.npy')  \n",
    "\n",
    "# charger des images  (5547,1); (0 = no cancer, 1 = cancer)\n",
    "Y = np.load('D:/test cancer rbreast/p/zoom_40m_label.npy')   \n",
    "perm_array = np.arange(len(X))\n",
    "#arrange genere tous toute les val jusqua la longuer de x_images\n",
    "np.random.shuffle(perm_array)\n",
    "#il melange les images\n",
    "X = X[perm_array]\n",
    "Y = Y[perm_array]\n",
    "#en split notre dataset 80 % 20 %\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# en redduit la taille de l'echantillon au cas ou en avais trop d'images \n",
    "#pour pas que ca prend trop de temps pour le pc \n",
    "#mais ici c'est pas le cas donc en commente ce code\n",
    "#X_train = X_train[0:30000] \n",
    "#Y_train = Y_train[0:30000]\n",
    "#X_test = X_test[0:30000] \n",
    "#Y_test = Y_test[0:30000]\n",
    "\n",
    "# normalizer nos donnees \n",
    "#en pourrais utiliser d'autre methodes aussi mais celle la c'est bien \n",
    "X_train = X_train / 256.0\n",
    "X_test = X_test / 256.0\n",
    "#methode qu'on a trouver sur internet et qu'on a utiliser \n",
    "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
    "#en applatit les donnees en gros en les transfrome en une matrice 1 seul dimmenssion nbr elem*(width*height*channel)\n",
    "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
    "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\n",
    "Y_train = to_categorical(Y_train, num_classes = 2)\n",
    "Y_test = to_categorical(Y_test, num_classes = 2)\n",
    "def plotKerasLearningCurve():\n",
    "    #en affiche juste la courbe de notre model\n",
    "    plt.figure(figsize=(10,5))\n",
    "    metrics = np.load('logs.npy')[()]\n",
    "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
    "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "        l = np.array(metrics[k])\n",
    "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
    "        y = l[x]\n",
    "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
    "    plt.legend(loc=4)\n",
    "    plt.axis([0, None, None, None]);\n",
    "    plt.grid()\n",
    "    plt.xlabel('nombre d epoques')\n",
    "\n",
    "def runKerasCNN(a,b,c,d):\n",
    "    \"\"\"\n",
    "    en a utiliser ce lien\n",
    "    https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "    en a utilise run model qui marche bien sur MNIST sur notre dataset\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 40\n",
    "    img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    x_train = a\n",
    "    y_train = b\n",
    "    x_test = c\n",
    "    y_test = d   \n",
    "    model = Sequential()\n",
    "    #test d'un premier model\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu', input_shape = input_shape))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = \"softmax\"))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "     # checkpoint\n",
    "    filepath=\"weights.traint_40zoom_modelcomplex1.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history =model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),callbacks=callbacks_list)\n",
    "    model.summary()\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('\\nKeras CNN 1 - accuracy:', score[1],'\\n')\n",
    "    y_pred = model.predict(c) \n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')\n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "runKerasCNN(X_train, Y_train,  X_test, Y_test)\n",
    "#plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
